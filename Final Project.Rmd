
```{r setup, include = FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(knitr)
library(purrr)
library(cluster) 
library(gridExtra)
library(grid)
library(NbClust)
library(factoextra)
```

```{r, include = TRUE}
#1)	Discuss the business problem/goal

#The goal of this project is to segment the customers based on the age, gender, interest. Customer segmentation is an important practice of dividing customers base into individual groups that are similar. It is useful in customized marketing.

#Customer Segmentation is one the most important applications of unsupervised learning. Using clustering techniques, companies can identify the several segments of customers allowing them to target the potential user base. In this machine learning project, we will make use of K-means clustering which is the essential algorithm for clustering unlabeled dataset.
```

```{r, include=TRUE}
#2)	Identify where the dataset was retrieved from 

#The data was collected from the below links which has the data link and source code
#https://data-flair.training/blogs/r-data-science-project-customer-segmentation/
#https://www.kaggle.com/shwetabh123/mall-customers
```


```{r, include = TRUE}

# 3) Identify the code that imported and saved your dataset in R 
customer_data=read.csv("E:/McDaniel/ANA 515/Mall_Customers.csv")
str(customer_data)
names(customer_data)

```

```{r, include = TRUE}
# 4)	Describe your data set 
# calculating number of Rows
nrow(customer_data)
#calculating number of Columns
ncol(customer_data)
print(customer_data)
#calculating mean
mean(customer_data$Age)
#calculating standard deviation
sd(customer_data$Age)
#calculating summary
summary(customer_data)

```

```{r, include = TRUE}
#5)	Discuss any data preparation, missing values and errors 
# The dataset has gender, customer id, age, annual income, and spending score columns and it looks complete and I do not think any values are missing as it is kept simple and effective. 

#I am not sure how the annual income of the customers were collected and on what basis and this is the only error with the data I could think of. 
```

```{r, include = TRUE}
#6)	discuss the modeling

#Using the k-means clustering algorithm, the first step is to indicate the number of clusters (k) that we wish to produce in the final output. The algorithm starts by selecting k objects from dataset randomly that will serve as the initial centers for our clusters. These selected objects are the cluster means, also known as centroids. Then, the remaining objects have an assignment of the closest centroid. This centroid is defined by the Euclidean Distance present between the object and the cluster mean. We refer to this step as “cluster assignment”. 

#When the assignment is complete, the algorithm proceeds to calculate new mean value of each cluster present in the data. After the recalculation of the centers, the observations are checked if they are closer to a different cluster. Using the updated cluster mean, the objects undergo reassignment. This goes on repeatedly through several iterations until the cluster assignments stop altering. The clusters that are present in the current iteration are the same as the ones obtained in the previous iteration.

```

```{r, include = TRUE}
# 7.produce and discuss the output 

#The main goal behind cluster partitioning methods like k-means is to define the clusters such that the intra-cluster variation stays minimum.

set.seed(123)
# function to calculate total intra-cluster sum of square 
iss <- function(k) {
  kmeans(customer_data[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}
k.values <- 1:10
iss_values <- map_dbl(k.values, iss)
plot(k.values, iss_values,
    type="b", pch = 19, frame = FALSE, 
    xlab="Number of clusters K",
    ylab="Total intra-clusters sum of squares")

#From the graph, we conclude that 4 is the appropriate number of clusters since it seems to be appearing at the bend in the elbow plot.

#Using average silhouette method, we can measure the quality of our clustering operation. With this, we can determine how well within the cluster is the data object. If we obtain a high average silhouette width, it means that we have good clustering. The average silhouette method calculates the mean of silhouette observations for different k values. With the optimal number of k clusters, one can maximize the average silhouette over significant values for k clusters.

#Using the silhouette function in the cluster package, we can compute the average silhouette width using the kmean function. Here, the optimal cluster will possess highest average.


k2<-kmeans(customer_data[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(customer_data[,3:5],"euclidean")))

k3<-kmeans(customer_data[,3:5],3,iter.max=100,nstart=50,algorithm="Lloyd")
s3<-plot(silhouette(k3$cluster,dist(customer_data[,3:5],"euclidean")))

k4<-kmeans(customer_data[,3:5],4,iter.max=100,nstart=50,algorithm="Lloyd")
s4<-plot(silhouette(k4$cluster,dist(customer_data[,3:5],"euclidean")))

k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],"euclidean")))

k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],"euclidean")))

k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))

k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))

k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],"euclidean")))

k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],"euclidean")))

#Now, we make use of the fviz_nbclust() function to determine and visualize the optimal number of clusters as follows –

fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")

#For computing the gap statistics method we can utilize the clusGap function for providing gap statistic as well as standard error for a given output.
#To compute gap statistic
set.seed(125)
stat_gap <- clusGap(customer_data[,3:5], FUN = kmeans, nstart = 25,
            K.max = 10, B = 50)
fviz_gap_stat(stat_gap)

#Now, let us take k = 6 as our optimal cluster
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
k6

#In the output of our kmeans operation, we observe a list with several key information.

```


```{r, include = TRUE}
# 8.provide explanation with any visuals 

#Visualizing the Clustering Results using the First Two Principle Components

pcclust=prcomp(customer_data[,3:5],scale=FALSE) #principal component analysis
summary(pcclust)
pcclust$rotation[,1:2]
set.seed(1)
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")

#From the above visualization, we observe that there is a distribution of 6 clusters as follows –

#Cluster 6 and 4 – These clusters represent the customer_data with the medium income salary as well as the medium annual spend of salary.

#Cluster 1 – This cluster represents the customer_data having a high annual income as well as a high annual spend.

#Cluster 3 – This cluster denotes the customer_data with low annual income as well as low yearly spend of income.

#Cluster 2 – This cluster denotes a high annual income and low yearly spend.

#Cluster 5 – This cluster represents a low annual income but its high yearly expenditure.

ggplot(customer_data, aes(x =Spending.Score..1.100., y =Age)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
                      breaks=c("1", "2", "3", "4", "5","6"),
                      labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")

kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}
digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters
plot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab ="K-means",ylab="classes")
legend("bottomleft",unique(dignm),fill=unique(kCols(digCluster)))

#Cluster 4 and 1 – These two clusters consist of customers with medium PCA1 and medium PCA2 score.

#Cluster 6 – This cluster represents customers having a high PCA2 and a low PCA1.

#Cluster 5 – In this cluster, there are customers with a medium PCA1 and a low PCA2 score.

#Cluster 3 – This cluster comprises of customers with a high PCA1 income and a high PCA2.

#Cluster 2 – This comprises of customers with a high PCA2 and a medium annual spend of income.

#With the help of clustering, we can understand the variables much better, prompting us to take careful decisions. With the identification of customers, companies can release products and services that target customers based on several parameters like income, age, spending patterns, etc.

```










